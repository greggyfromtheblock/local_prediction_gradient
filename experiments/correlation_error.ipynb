{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ruyogagp/medical_interpretability')\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from cga import cga\n",
    "import pandas as pd\n",
    "import neptune.new as neptune\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from source.tasks import DeepSurv\n",
    "\n",
    "from captum.attr import *\n",
    "from captum.metrics import *\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression\n",
    "from source.wrappers import ForwardWrapper\n",
    "import os\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "RESULTS_DIR = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/results/interpretability/resample_multiplicities'\n",
    "DATA_DIR = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load from wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# TODO: this works if the second tag is the experiment identifier\n",
    "def load_details(run):\n",
    "    dir = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities'\n",
    "    path = f'{dir}/{run.tags[0]}_attribute_details.csv'\n",
    "    details_df = pd.read_csv(path)\n",
    "    return details_df, path\n",
    "\n",
    "def output_diff(model, orig_features, resampled_features):\n",
    "    orig_output, *_ = model(orig_features)\n",
    "    resampled_output, *_ = model(resampled_features)\n",
    "    with torch.no_grad():\n",
    "        diff = torch.sub(orig_output, resampled_output)\n",
    "    return diff.detach().numpy()\n",
    "\n",
    "def load_attributions(run, method):\n",
    "    attr_x = np.genfromtxt(run.config[f'{method}_x_path'], delimiter=',')\n",
    "    attr_y = np.genfromtxt(run.config[f'{method}_y_path'], delimiter=',')\n",
    "    return attr_x, attr_y\n",
    "\n",
    "def calculate_error(details_x, details_y):\n",
    "    for detail in [details_x, details_y]:\n",
    "        outlier_cs = np.percentile(detail.change_slope, [0.5, 99.5])\n",
    "        clipped_cs = np.clip(detail.change_slope.to_numpy(), *outlier_cs)\n",
    "        cs_norm = clipped_cs / abs(clipped_cs).max()\n",
    "        detail.loc[:,'change_slope_norm'] = cs_norm\n",
    "\n",
    "        outlier_attr = np.percentile(detail.attribution, [0.5, 99.5])\n",
    "        clipped_attr = np.clip(detail.attribution.to_numpy(), *outlier_attr)\n",
    "        detail.loc[:,'attribution_norm'] = clipped_attr / abs(clipped_attr).max()\n",
    "        detail.loc[:, 'error'] = abs(detail.attribution_norm - detail.change_slope_norm)\n",
    "\n",
    "def view_error(project, id, method):\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"cardiors/{project}/{id}\")\n",
    "    x, y = change_slope_singular(run, method)\n",
    "    y.plot.scatter('x_orig', 'y_orig', c='error', cmap='PuBu')\n",
    "    x.plot.scatter('x_orig', 'y_orig', c='error', cmap='PuBu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def change_slope_singular(run, method):\n",
    "    \"\"\"\n",
    "    :param run: wandb run object\n",
    "    :param method: attribution method to calculate change_slope for\n",
    "    :return: None -> loads the details dataframe of the experiment and writes the change slope dataframe to {experiment}/{change_slope}/x_{method}_{seed}.csv\n",
    "    \"\"\"\n",
    "    details = load_details(run)\n",
    "    model = DeepSurv.load_from_checkpoint(run.config['checkpoint_path'])\n",
    "    details_x = details[details['modified_attribute'] == 'noise_x']\n",
    "    details_y = details[details['modified_attribute'] == 'noise_y']\n",
    "    attr_x, attr_y = load_attributions(run, method)\n",
    "    for detail in [details_x, details_y]:\n",
    "        # get input for model difference\n",
    "        original_values = torch.Tensor(detail[['x_orig', 'y_orig']].to_numpy(dtype='float32'))\n",
    "        # TODO: loop and change per intervention id, also store intermediate results in a dictionary\n",
    "        resampled_values = torch.Tensor(detail[['x_do', 'y_do']].to_numpy(dtype='float32'))\n",
    "        # get resampling difference based on the modified attribute\n",
    "        if detail.loc[detail.index.min(), ['modified_attribute']].item() == 'noise_x':\n",
    "            # TODO: Store resampling difference in dict {resampling_difference0, resampling_difference1, etc}\n",
    "            detail.loc[:, 'resampling_diff'] = original_values[:, 0] - resampled_values[:, 0]\n",
    "            #TODO: fix this\n",
    "            detail.loc[:, 'attribution'] = attr_x[:, 0]\n",
    "        elif detail.loc[detail.index.min(), ['modified_attribute']].item() == 'noise_y':\n",
    "            detail.loc[:, 'resampling_diff'] = original_values[:, 1] - resampled_values[:, 1]\n",
    "            detail.loc[:, 'attribution'] = attr_y[:, 1]\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "        # calculate change slope\n",
    "\n",
    "        detail.loc[:, 'model_diff']= output_diff(model, original_values, resampled_values)\n",
    "        detail.loc[:, 'change_slope'] = detail.model_diff / detail.resampling_diff\n",
    "    calculate_error(details_x, details_y)\n",
    "\n",
    "    # save dataframes\n",
    "    seed = eval(run.config['_content']['experiment'])['datamodule_kwargs']['seed']\n",
    "    filepath_x = '/'.join(run.config[f'{method}_x_path'].split('/')[0:-1] + ['change_slope', f'x_{method}_{seed}.csv'])\n",
    "    filepath_y = '/'.join(run.config[f'{method}_x_path'].split('/')[0:-1] + ['change_slope', f'y_{method}_{seed}.csv'])\n",
    "    details_x.to_csv(filepath_x)\n",
    "    details_y.to_csv(filepath_y)\n",
    "\n",
    "    return details_x, details_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_mean_error(experiment_id, method):\n",
    "    \"\"\"\n",
    "    :param experiment_id: experiment id\n",
    "    :param method: attribution method\n",
    "    :return: None -> prints the error graph\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs =  api.runs('cardiors/interpretability',\n",
    "                     filters={\"$and\": [{'tags': f'{experiment_id}'}, {'state': 'finished'}]})\n",
    "    print('len runs:', len(runs))\n",
    "    for run in runs:\n",
    "        deets_x, deets_y = change_slope_singular(run, method)\n",
    "\n",
    "\n",
    "    valid_attr_x = []\n",
    "    valid_attr_y = []\n",
    "    dir = os.listdir(f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/results/interpretability/correlation_case/{experiment_id}/change_slope/')\n",
    "    change_slope_path = f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/results/interpretability/correlation_case/{experiment_id}/change_slope/'\n",
    "    for attr in dir:\n",
    "        dim = attr.split('_')[0]\n",
    "        name = attr.split('_')[1]\n",
    "        seed = attr.split('_')[2]\n",
    "        if dim == 'x' and name == method:\n",
    "            valid_attr_x.append(pd.read_csv(f'{change_slope_path}/{attr}', index_col=0))\n",
    "        elif dim == 'y' and name == method:\n",
    "            valid_attr_y.append(pd.read_csv(f'{change_slope_path}/{attr}', index_col=0))\n",
    "\n",
    "    err_x = None\n",
    "    err_y = None\n",
    "    for attr in valid_attr_x:\n",
    "        error_x = attr['error'].to_numpy().reshape(-1, 1)\n",
    "        err_x = error_x if err_x is None else np.concatenate((err_x, error_x), axis=1)\n",
    "    mean_error_x = np.mean(err_x, axis=1)\n",
    "    deets_x.loc[:, 'mean_error'] = mean_error_x\n",
    "\n",
    "    for attr in valid_attr_y:\n",
    "        error_y = attr['error'].to_numpy().reshape(-1, 1)\n",
    "        if np.isnan(error_y).any():\n",
    "            continue\n",
    "        err_y = error_y if err_y is None else np.concatenate((err_y, error_y), axis=1)\n",
    "    mean_error_y = np.mean(err_y, axis=1)\n",
    "    deets_y.loc[:, 'mean_error'] = mean_error_y\n",
    "\n",
    "    # plot figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    fig.suptitle(method, fontsize=35)\n",
    "    dim0_mean = deets_x['mean_error'].mean()\n",
    "    dim1_mean = deets_y['mean_error'].mean()\n",
    "    ax1.set_title(f\"dimension 0 error: {dim0_mean}\", fontdict=dict(fontsize=20))\n",
    "    ax2.set_title(f\"dimension 1 error: {dim1_mean}\", fontdict=dict(fontsize=20))\n",
    "    deets_x.plot.scatter('x_orig', 'y_orig', c='mean_error', cmap='PuBu', vmax=1.9, vmin=0.0, ax=ax1)\n",
    "    deets_y.plot.scatter('x_orig', 'y_orig', c='mean_error', cmap='PuBu', vmax=1.9, vmin=0.0, ax=ax2)\n",
    "    deets_x.to_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv')\n",
    "    deets_y.to_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv')\n",
    "\n",
    "    return deets_x, deets_y, err_x, err_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multipoint-resampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def restructure_dfs(experiment_id):\n",
    "    \"\"\"\n",
    "    Restructure details dataframe to have the resampling points as a column, also adds in a column of resampling difference\n",
    "    :param experiment_id: experiment identifier to use\n",
    "    :return: None -> saves the resampling dataframe as {experiment_id}_resample_[x,y].csv\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    runs =  api.runs('cardiors/interpretability',\n",
    "                     filters={\"$and\": [{'tags': f'{experiment_id}'}, {'tags': 'resample_multiplicities'}, {'state': 'finished'}]})\n",
    "    run = runs[0]\n",
    "    details_df, path = load_details(run)\n",
    "\n",
    "    # NOISE X INTERVENTION\n",
    "    dfx = details_df[details_df.modified_attribute == 'noise_x']\n",
    "    x_colnames = [f'x_intervention{n}' for n in range(100)]\n",
    "    intervention_xdf = dfx.loc[:, x_colnames]\n",
    "    intervention_xdf['x_intervention'] = intervention_xdf.values.tolist()\n",
    "    resampling_xdf = pd.DataFrame(dict(x_orig=dfx.x_orig, y_orig=dfx.y_orig, x_resampling=intervention_xdf.x_intervention, time=dfx.time_orig, event=dfx.event_orig))\n",
    "\n",
    "    # Resampling difference noise_x\n",
    "    orig_vals = resampling_xdf.loc[:, 'x_orig'].values\n",
    "    resamp_vals = resampling_xdf.loc[:, 'x_resampling'].values.tolist()\n",
    "    resamp_vals_numpy=np.array([np.array(row) for row in resamp_vals])\n",
    "    orig_vals_array = np.array([[val] * 100 for val in orig_vals])\n",
    "    resampling_xdf.loc[:, 'resampling_diff'] = np.absolute(orig_vals_array - resamp_vals).tolist()\n",
    "    resampling_xdf.to_csv(f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/{experiment_id}_resampling_x.csv')\n",
    "\n",
    "    # NOISE Y INTERVENTION\n",
    "    dfy = details_df[details_df.modified_attribute == 'noise_y']\n",
    "    y_colnames = [f'y_intervention{n}' for n in range(100)]\n",
    "    intervention_ydf = dfy.loc[:, y_colnames]\n",
    "    intervention_ydf['y_intervention'] = intervention_ydf.values.tolist()\n",
    "    resampling_ydf = pd.DataFrame(dict(x_orig=dfy.x_orig, y_orig=dfy.y_orig, y_resampling=intervention_ydf.y_intervention, time=dfy.time_orig, event=dfy.event_orig))\n",
    "\n",
    "    # Resampling difference noise_y\n",
    "    orig_vals = resampling_ydf.loc[:, 'y_orig'].values\n",
    "    resamp_vals = resampling_ydf.loc[:, 'y_resampling'].values.tolist()\n",
    "    resamp_vals_numpy=np.array([np.array(row) for row in resamp_vals])\n",
    "    orig_vals_array = np.array([[val] * 100 for val in orig_vals])\n",
    "    resampling_ydf.loc[:, 'resampling_diff'] = np.absolute(orig_vals_array - resamp_vals).tolist()\n",
    "    resampling_ydf.to_csv(f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/{experiment_id}_resampling_y.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_ids = ['p0.00', 'p0.25', 'p0.5', 'p0.75']\n",
    "for experiment_id in experiment_ids:\n",
    "    restructure_dfs(experiment_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change Slope"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# CREATE TUPLES FOR MODEL DIFF\n",
    "\n",
    "def col2numpy(df, colname):\n",
    "    \"\"\"\n",
    "    convert a dataframe column to a numpy array\n",
    "    :param df: dataframe\n",
    "    :param colname: column name to convert to numpy array\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vals = df.loc[:, colname].values.tolist()\n",
    "    vals_numpy = np.array([np.array(eval(row)) for row in vals])\n",
    "    return vals_numpy\n",
    "\n",
    "def clip_norm2d(array):\n",
    "    \"\"\"\n",
    "    clips and normalizes the array to the range of [-1, 1]\n",
    "    :param array: array to be normalized\n",
    "    :return: normalized array\n",
    "    \"\"\"\n",
    "    array_norm = np.zeros((array.shape))\n",
    "    for i in range(array.shape[1]):\n",
    "        column = array[:, i]\n",
    "        outliers = np.percentile(column, [0.5, 99.5])\n",
    "        clipped_array = np.clip(column, *outliers)\n",
    "        col_norm = clipped_array / abs(clipped_array).max()\n",
    "        #TODO: !!!!!FIX THIS!!!!\n",
    "        array_norm[:, i] = col_norm\n",
    "    return  array_norm\n",
    "\n",
    "def clip_norm1d(array):\n",
    "    \"\"\"\n",
    "    clips and normalizes the array to the range of [-1, 1]\n",
    "    :param array:  array to be normalized\n",
    "    :return: normalized array\n",
    "    \"\"\"\n",
    "    outliers = np.percentile(array, [0.5, 99.5])\n",
    "    clipped_array = np.clip(array, *outliers)\n",
    "    array_norm = clipped_array / abs(clipped_array).max()\n",
    "    return array_norm\n",
    "\n",
    "def get_errors(change_slope_array, attr_array):\n",
    "    \"\"\"\n",
    "    calculates squared & absolute error between the attribution value and change_slope\n",
    "    :param change_slope_array: change slope array (2 dimensional)\n",
    "    :param attr_array: attribution values array (1 dimensional)\n",
    "    :return: squared_error, absolute error (both 2 dimensional)\n",
    "    \"\"\"\n",
    "    squared_error = np.zeros((change_slope_array.shape))\n",
    "    absolute_error = np.zeros((change_slope_array.shape))\n",
    "\n",
    "    for j in range(change_slope_array.shape[0]):\n",
    "        for i in range(change_slope_array.shape[1]):\n",
    "            attr = attr_array[i]\n",
    "            cs = change_slope_array[j, i]\n",
    "            squared_error[j, i] = np.square(attr - cs)\n",
    "            absolute_error[j, i] = np.absolute(attr - cs)\n",
    "\n",
    "    mae = np.mean(absolute_error, axis=1)\n",
    "    mse = np.mean(squared_error, axis=1)\n",
    "    rmse = np.sqrt(np.mean(squared_error, axis=1))\n",
    "    return squared_error, absolute_error, rmse, mse, mae"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def resampling_error(experiment_id, method):\n",
    "    \"\"\"\n",
    "    creates a dataframe with resampling error for each point\n",
    "    :param experiment_id: experiment identifier\n",
    "    :param method: feature ablation method to calculate errors on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_id = experiment_id\n",
    "    api = wandb.Api()\n",
    "    runs =  api.runs('cardiors/interpretability',\n",
    "                     filters={\"$and\": [{'tags': f'{experiment_id}'}, {'tags': 'resample_multiplicities'}, {'state': 'finished'}]})\n",
    "\n",
    "    # For testing purpose\n",
    "    runs = [runs[0]]\n",
    "    for run in runs:\n",
    "        # load resampling dataframe and model\n",
    "        xdf = pd.read_csv(f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/{experiment_id}_resampling_x.csv', index_col=0)\n",
    "        ydf = pd.read_csv(f'/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/{experiment_id}_resampling_y.csv', index_col=0)\n",
    "        model = DeepSurv.load_from_checkpoint(run.config['checkpoint_path'])\n",
    "\n",
    "        # resampling tensors for xdf\n",
    "        orig_tensor_x = torch.Tensor(xdf[['x_orig', 'y_orig']].to_numpy(dtype='float64'))\n",
    "        resamp_vals = col2numpy(xdf, 'x_resampling')\n",
    "        constant_val = xdf['y_orig'].to_numpy(dtype='float64')\n",
    "        resamp_tensors_x = []\n",
    "        for i in range(resamp_vals.shape[1]):\n",
    "            # modified attribute == noise_x -> resamp_tensor = (resamp, constant)\n",
    "            resamp_tensor = torch.Tensor(np.concatenate((resamp_vals[:, i].reshape(-1, 1), constant_val.reshape(-1, 1)), axis=1))\n",
    "            resamp_tensors_x.append(resamp_tensor)\n",
    "\n",
    "        orig_tensor_y = torch.Tensor(ydf[['x_orig', 'y_orig']].to_numpy(dtype='float64'))\n",
    "        # resampling tensors for ydf\n",
    "        resamp_vals = col2numpy(ydf, 'y_resampling')\n",
    "        constant_val = ydf['x_orig'].to_numpy(dtype='float64')\n",
    "        resamp_tensors_y = []\n",
    "        for i in range(resamp_vals.shape[1]):\n",
    "            # modified attribute == noise_y -> resamp_tensor = (constant, resamp)\n",
    "            resamp_tensor = torch.Tensor(np.concatenate((constant_val.reshape(-1, 1), resamp_vals[:, i].reshape(-1, 1)), axis=1))\n",
    "            resamp_tensors_y.append(resamp_tensor)\n",
    "\n",
    "        # CALCULATE MODEL DIFF + CHANGE SLOPE\n",
    "\n",
    "        # load attributions\n",
    "        attr_x, attr_y = load_attributions(run, method=method)\n",
    "        attr_x = attr_x[:, 0]\n",
    "        attr_y = attr_y[:, 1]\n",
    "\n",
    "        orig_output = model(orig_tensor_x)[0].detach().numpy()\n",
    "        resamp_outputs = None\n",
    "        for i in range(resamp_vals.shape[1]):\n",
    "            resamp_output = model(resamp_tensors_x[i])[0].detach().numpy()\n",
    "            resamp_outputs = resamp_output if resamp_outputs is None else np.concatenate((resamp_outputs, resamp_output), axis=1)\n",
    "\n",
    "        # calculate change slope\n",
    "        model_diff = np.subtract(orig_output, resamp_outputs)\n",
    "        resampling_diff = col2numpy(xdf, 'resampling_diff')\n",
    "        change_slope = np.true_divide(model_diff, resampling_diff)\n",
    "        mean_cs = np.mean(change_slope, axis=1)\n",
    "\n",
    "        xdf.loc[:, 'model_diff'] = model_diff.tolist()\n",
    "        xdf.loc[:, 'change_slope'] = change_slope.tolist()\n",
    "        xdf.loc[:, 'mean_cs'] = mean_cs\n",
    "        xdf.loc[:, 'attribution_x'] = attr_x\n",
    "\n",
    "        # calculate change slope\n",
    "        orig_output = model(orig_tensor_y)[0].detach().numpy()\n",
    "        resamp_outputs = None\n",
    "        for i in range(resamp_vals.shape[1]):\n",
    "            resamp_output = model(resamp_tensors_y[i])[0].detach().numpy()\n",
    "            resamp_outputs = resamp_output if resamp_outputs is None else np.concatenate((resamp_outputs, resamp_output), axis=1)\n",
    "\n",
    "        model_diff = np.subtract(orig_output, resamp_outputs)\n",
    "        resampling_diff = col2numpy(ydf, 'resampling_diff')\n",
    "        change_slope = np.true_divide(model_diff, resampling_diff)\n",
    "        mean_cs = np.mean(change_slope, axis=1)\n",
    "\n",
    "        ydf.loc[:, 'model_diff'] = model_diff.tolist()\n",
    "        ydf.loc[:, 'change_slope'] = change_slope.tolist()\n",
    "        ydf.loc[:, 'mean_cs'] = mean_cs\n",
    "        ydf.loc[:, 'attribution_y'] = attr_y\n",
    "\n",
    "        # Save dataframes\n",
    "        EVALUATION_DIR = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/results/interpretability/resample_multiplicities/evaluation'\n",
    "        if not os.path.exists(f'{EVALUATION_DIR}/{experiment_id}'):\n",
    "            os.mkdir(f'{EVALUATION_DIR}/{experiment_id}')\n",
    "\n",
    "        if not os.path.exists(f'{EVALUATION_DIR}/{experiment_id}/change_slope'):\n",
    "            os.mkdir(f'{EVALUATION_DIR}/{experiment_id}/change_slope')\n",
    "\n",
    "        seed = eval(run.config['_content']['experiment'])['datamodule_kwargs']['seed']\n",
    "        xdf.to_csv(f'{EVALUATION_DIR}/{experiment_id}/change_slope/x_{method}_{seed}.csv')\n",
    "        ydf.to_csv(f'{EVALUATION_DIR}/{experiment_id}/change_slope/y_{method}_{seed}.csv')\n",
    "        return xdf, ydf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "methods = ['FeatureAblation',\n",
    "           'KernelExplainer',\n",
    "           'DeepExplainer']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "experiment_ids = ['p0.75']\n",
    "xdf, ydf = resampling_error(experiment_id, 'FeatureAblation')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "        x_orig    y_orig                                       x_resampling  \\\n0     0.348720  0.389544  [-0.3359962421121104, 0.0757522714949251, -0.8...   \n1     0.784569  0.446195  [-0.1530040531730433, 1.087602749330351, 0.819...   \n2     1.209356  1.314928  [0.9367276467400668, 0.1407234548299741, 0.212...   \n3     0.233735 -0.274223  [-1.7405027677917992, -0.5069073641056587, 0.0...   \n4    -0.612208  0.822774  [1.7694521637651923, -0.4403750472000197, -1.4...   \n...        ...       ...                                                ...   \n5095 -2.713351 -2.496023  [0.0105750265863676, -0.7937752253749618, -0.4...   \n5096  0.309199 -0.334600  [1.4345700560264594, 1.6163265076751128, -0.15...   \n5097  0.748925  0.182532  [-0.0265191369169419, -0.3005558763207619, 0.5...   \n5098 -0.567965 -0.505648  [-0.5771746694021446, -0.4294065409476704, -1....   \n5099 -0.154772  0.914701  [-0.7535308291284383, -1.0136697302285074, 0.1...   \n\n          time  event                                    resampling_diff  \\\n0     0.791166    1.0  [0.6847161281312388, 0.2729676145242033, 1.215...   \n1     0.806355    1.0  [0.9375727475942667, 0.3030340549091277, 0.034...   \n2     0.000000    0.0  [0.2726279408110882, 1.068632132721181, 0.9970...   \n3     0.878572    1.0  [1.9742376039544363, 0.7406422002682957, 0.171...   \n4     0.920771    1.0  [2.381660448933591, 0.17183323796837907, 0.837...   \n...        ...    ...                                                ...   \n5095  1.142518    1.0  [2.7239258022415243, 1.9195755502801948, 2.251...   \n5096  0.842824    1.0  [1.1253710448337375, 1.307127496482391, 0.4632...   \n5097  0.307402    1.0  [0.775444065070997, 1.049480804474817, 0.18374...   \n5098  0.895207    1.0  [0.00920977065479156, 0.13855835779968262, 1.1...   \n5099  0.849427    1.0  [0.5987590770616844, 0.8588979781617536, 0.304...   \n\n                                             model_diff  \\\n0     [0.24990850687026978, 0.0982816070318222, 0.47...   \n1     [0.3737141489982605, -0.1337873935699463, -0.0...   \n2     [0.0038338899612426758, 0.3645629286766052, 0....   \n3     [0.8953638076782227, 0.34671059250831604, 0.10...   \n4     [-1.1232879161834717, -0.14560528099536896, 0....   \n...                                                 ...   \n5095  [-0.9708305597305298, -0.621554970741272, -0.7...   \n5096  [-0.4605836868286133, -0.5177865028381348, 0.2...   \n5097  [0.3347215950489044, 0.46269601583480835, 0.09...   \n5098  [0.03597268462181091, -0.012654811143875122, 0...   \n5099  [0.1661377251148224, 0.2972027659416199, -0.21...   \n\n                                           change_slope   mean_cs  \\\n0     [0.36498118943441926, 0.3600485984505236, 0.38...  0.064747   \n1     [0.39859749545534445, -0.44149293256847244, -0...  0.143162   \n2     [0.014062718406031937, 0.34114913590355617, 0....  0.163415   \n3     [0.45352383415491204, 0.46812157392965326, 0.5...  0.201644   \n4     [-0.47164066426279766, -0.8473638902280558, 0.... -0.406306   \n...                                                 ...       ...   \n5095  [-0.3564085919416863, -0.3237981285240612, -0.... -0.351507   \n5096  [-0.40927273626154115, -0.39612547684257987, 0...  0.252234   \n5097  [0.4316515015409377, 0.4408808754404532, 0.514...  0.326889   \n5098  [3.905926213601794, -0.09133199429348396, 0.47... -0.016208   \n5099  [0.2774700734895194, 0.34602801904098623, -0.7... -0.478636   \n\n      attribution_x  \n0          0.115524  \n1          0.271590  \n2          0.409826  \n3          0.079062  \n4         -0.199007  \n...             ...  \n5095      -0.945044  \n5096       0.106218  \n5097       0.262278  \n5098      -0.191115  \n5099      -0.051256  \n\n[5100 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x_orig</th>\n      <th>y_orig</th>\n      <th>x_resampling</th>\n      <th>time</th>\n      <th>event</th>\n      <th>resampling_diff</th>\n      <th>model_diff</th>\n      <th>change_slope</th>\n      <th>mean_cs</th>\n      <th>attribution_x</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.348720</td>\n      <td>0.389544</td>\n      <td>[-0.3359962421121104, 0.0757522714949251, -0.8...</td>\n      <td>0.791166</td>\n      <td>1.0</td>\n      <td>[0.6847161281312388, 0.2729676145242033, 1.215...</td>\n      <td>[0.24990850687026978, 0.0982816070318222, 0.47...</td>\n      <td>[0.36498118943441926, 0.3600485984505236, 0.38...</td>\n      <td>0.064747</td>\n      <td>0.115524</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.784569</td>\n      <td>0.446195</td>\n      <td>[-0.1530040531730433, 1.087602749330351, 0.819...</td>\n      <td>0.806355</td>\n      <td>1.0</td>\n      <td>[0.9375727475942667, 0.3030340549091277, 0.034...</td>\n      <td>[0.3737141489982605, -0.1337873935699463, -0.0...</td>\n      <td>[0.39859749545534445, -0.44149293256847244, -0...</td>\n      <td>0.143162</td>\n      <td>0.271590</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.209356</td>\n      <td>1.314928</td>\n      <td>[0.9367276467400668, 0.1407234548299741, 0.212...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>[0.2726279408110882, 1.068632132721181, 0.9970...</td>\n      <td>[0.0038338899612426758, 0.3645629286766052, 0....</td>\n      <td>[0.014062718406031937, 0.34114913590355617, 0....</td>\n      <td>0.163415</td>\n      <td>0.409826</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.233735</td>\n      <td>-0.274223</td>\n      <td>[-1.7405027677917992, -0.5069073641056587, 0.0...</td>\n      <td>0.878572</td>\n      <td>1.0</td>\n      <td>[1.9742376039544363, 0.7406422002682957, 0.171...</td>\n      <td>[0.8953638076782227, 0.34671059250831604, 0.10...</td>\n      <td>[0.45352383415491204, 0.46812157392965326, 0.5...</td>\n      <td>0.201644</td>\n      <td>0.079062</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.612208</td>\n      <td>0.822774</td>\n      <td>[1.7694521637651923, -0.4403750472000197, -1.4...</td>\n      <td>0.920771</td>\n      <td>1.0</td>\n      <td>[2.381660448933591, 0.17183323796837907, 0.837...</td>\n      <td>[-1.1232879161834717, -0.14560528099536896, 0....</td>\n      <td>[-0.47164066426279766, -0.8473638902280558, 0....</td>\n      <td>-0.406306</td>\n      <td>-0.199007</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5095</th>\n      <td>-2.713351</td>\n      <td>-2.496023</td>\n      <td>[0.0105750265863676, -0.7937752253749618, -0.4...</td>\n      <td>1.142518</td>\n      <td>1.0</td>\n      <td>[2.7239258022415243, 1.9195755502801948, 2.251...</td>\n      <td>[-0.9708305597305298, -0.621554970741272, -0.7...</td>\n      <td>[-0.3564085919416863, -0.3237981285240612, -0....</td>\n      <td>-0.351507</td>\n      <td>-0.945044</td>\n    </tr>\n    <tr>\n      <th>5096</th>\n      <td>0.309199</td>\n      <td>-0.334600</td>\n      <td>[1.4345700560264594, 1.6163265076751128, -0.15...</td>\n      <td>0.842824</td>\n      <td>1.0</td>\n      <td>[1.1253710448337375, 1.307127496482391, 0.4632...</td>\n      <td>[-0.4605836868286133, -0.5177865028381348, 0.2...</td>\n      <td>[-0.40927273626154115, -0.39612547684257987, 0...</td>\n      <td>0.252234</td>\n      <td>0.106218</td>\n    </tr>\n    <tr>\n      <th>5097</th>\n      <td>0.748925</td>\n      <td>0.182532</td>\n      <td>[-0.0265191369169419, -0.3005558763207619, 0.5...</td>\n      <td>0.307402</td>\n      <td>1.0</td>\n      <td>[0.775444065070997, 1.049480804474817, 0.18374...</td>\n      <td>[0.3347215950489044, 0.46269601583480835, 0.09...</td>\n      <td>[0.4316515015409377, 0.4408808754404532, 0.514...</td>\n      <td>0.326889</td>\n      <td>0.262278</td>\n    </tr>\n    <tr>\n      <th>5098</th>\n      <td>-0.567965</td>\n      <td>-0.505648</td>\n      <td>[-0.5771746694021446, -0.4294065409476704, -1....</td>\n      <td>0.895207</td>\n      <td>1.0</td>\n      <td>[0.00920977065479156, 0.13855835779968262, 1.1...</td>\n      <td>[0.03597268462181091, -0.012654811143875122, 0...</td>\n      <td>[3.905926213601794, -0.09133199429348396, 0.47...</td>\n      <td>-0.016208</td>\n      <td>-0.191115</td>\n    </tr>\n    <tr>\n      <th>5099</th>\n      <td>-0.154772</td>\n      <td>0.914701</td>\n      <td>[-0.7535308291284383, -1.0136697302285074, 0.1...</td>\n      <td>0.849427</td>\n      <td>1.0</td>\n      <td>[0.5987590770616844, 0.8588979781617536, 0.304...</td>\n      <td>[0.1661377251148224, 0.2972027659416199, -0.21...</td>\n      <td>[0.2774700734895194, 0.34602801904098623, -0.7...</td>\n      <td>-0.478636</td>\n      <td>-0.051256</td>\n    </tr>\n  </tbody>\n</table>\n<p>5100 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.change_slope[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0        [0.26356083154678345]\n1         [0.2820919156074524]\n2        [0.14521509408950806]\n3       [-0.42148637771606445]\n4       [-0.24262291193008423]\n                 ...          \n5095     [-1.5800204277038574]\n5096     [-0.1761873960494995]\n5097      [1.0370593070983887]\n5098       [0.609470546245575]\n5099    [-0.06177254021167755]\nName: model_diff, Length: 5100, dtype: object"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.model_diff"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method = ['FeatureAblation']\n",
    "experiment_id = ['p0.70']\n",
    "xdf, ydf = resampling_error(experiment_id, method)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_ids = ['p0.75']\n",
    "for method in methods:\n",
    "    for experiment_id in experiment_ids:\n",
    "        resampling_error(experiment_id, method)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "deets_x, deets_y, err_x, err_y = get_mean_error('p0.00', method='FeatureAblation')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(err_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "deets_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## p = 0.00"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "methods = ['FeatureAblation',\n",
    "           'FeaturePermutation',\n",
    "           'KernelExplainer',\n",
    "           'DeepExplainer',\n",
    "           'ShapleyValueSampling',\n",
    "           'IntegratedGradients',\n",
    "           'InputXGradient',\n",
    "           'Lime']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## p = -0.75"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    _ = get_mean_error(experiment_id='p-0.75', method=method)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method = 'KernelExplainer'\n",
    "\n",
    "def plot_distributions(method):\n",
    "    experiment_id = 'p0.00'\n",
    "    xdf_00 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_00 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_00, ydf_00]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.00\n",
    "\n",
    "    experiment_id = 'p-0.75'\n",
    "    xdf_75 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_75 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_75, ydf_75]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.75\n",
    "\n",
    "    experiment_id = 'p0.30'\n",
    "    xdf_30 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_30 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_30, ydf_30]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.30\n",
    "\n",
    "    experiment_id = 'p0.50'\n",
    "    xdf_50 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_50 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_50, ydf_50]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.50\n",
    "\n",
    "    experiment_id = 'p0.20'\n",
    "    xdf_20 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_20 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_20, ydf_20]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.20\n",
    "\n",
    "    full_xdf = xdf_75.append([xdf_00, xdf_20, xdf_30, xdf_50], ignore_index=True)\n",
    "    full_ydf = ydf_75.append([xdf_00, ydf_20, ydf_30, ydf_50], ignore_index=True)\n",
    "    full_df = full_xdf.append(full_ydf, ignore_index=True)\n",
    "    sns.set(rc={'figure.figsize':(20,10)})\n",
    "    sns.displot(data=full_df,\n",
    "                x='mean_error',\n",
    "                hue=\"correlation_coefficient\",\n",
    "                col='modified_attribute',\n",
    "                fill=True,\n",
    "                kind='kde',\n",
    "                palette='crest',\n",
    "                alpha=0.1,\n",
    "                linewidth=2.0,\n",
    "                legend=False)\n",
    "    plt.suptitle(method, y=0.99, x=0.54)\n",
    "plot_distributions(method)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    plot_distributions(method)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_df[full_df.modified_attribute=='noise_x' ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_df[full_df.attribution_method=='DeepExplainer'].nlargest(100, ['mean_error'])['x_orig'].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_id = 'p0.00'\n",
    "method = 'DeepExplainer'\n",
    "xdf_de = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "ydf_de = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "for df in [xdf_de, ydf_de]:\n",
    "    df.loc[:, 'attribution_method'] = method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise_ydf = full_df[full_df.modified_attribute == 'noise_y']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise_ydf[noise_ydf.attribution_method == 'DeepExplainer']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_df = compare_attributions('p-0.75', dimension='y')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "method = 'KernelExplainer'\n",
    "\n",
    "def compare_attributions(experiment_id:str, dimension:str = 'both') -> pd.DataFrame:\n",
    "    method = 'FeatureAblation'\n",
    "    xdf_fa = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_fa = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_fa, ydf_fa]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'FeaturePermutation'\n",
    "    xdf_fp = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_fp = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_fp, ydf_fp]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'KernelExplainer'\n",
    "    xdf_ke = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_ke = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_ke, ydf_ke]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'DeepExplainer'\n",
    "    xdf_de = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_de = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_de, ydf_de]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'ShapleyValueSampling'\n",
    "    xdf_svs = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_svs = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_svs, ydf_svs]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'IntegratedGradients'\n",
    "    xdf_ig = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_ig = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_ig, ydf_ig]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'InputXGradient'\n",
    "    xdf_ixg = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_ixg = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_ixg, ydf_ixg]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    method = 'Lime'\n",
    "    xdf_lime = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_lime = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_lime, ydf_lime]:\n",
    "        df.loc[:, 'attribution_method'] = method\n",
    "\n",
    "    full_xdf = xdf_de.append([xdf_lime, xdf_ixg, xdf_ig, xdf_fa, xdf_fp, xdf_ke, xdf_svs], ignore_index=True)\n",
    "    full_ydf = ydf_de.append([ydf_lime, ydf_ixg, ydf_ig, ydf_fa, ydf_fp, ydf_ke, ydf_svs], ignore_index=True)\n",
    "    full_df = full_xdf.append(full_ydf, ignore_index=True)\n",
    "\n",
    "    full_df = full_xdf if dimension=='x' else full_ydf if dimension=='y' else full_xdf.append(full_ydf, ignore_index=True)\n",
    "    sns.set(rc={'figure.figsize':(20,10)})\n",
    "    sns.boxplot(data=full_df,\n",
    "                x='attribution_method',\n",
    "                y=\"mean_error\",\n",
    "                hue='modified_attribute',\n",
    "                palette='Spectral_r')\n",
    "    plt.suptitle(experiment_id, y=0.99, x=0.54)\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def plot_distributions(method):\n",
    "    experiment_id = 'p0.00'\n",
    "    xdf_00 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_00 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_00, ydf_00]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.00\n",
    "\n",
    "    experiment_id = 'p-0.75'\n",
    "    xdf_75 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_75 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_75, ydf_75]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.75\n",
    "\n",
    "    experiment_id = 'p0.30'\n",
    "    xdf_30 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_30 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_30, ydf_30]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.30\n",
    "\n",
    "    experiment_id = 'p0.50'\n",
    "    xdf_50 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_50 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_50, ydf_50]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.50\n",
    "\n",
    "    experiment_id = 'p0.20'\n",
    "    xdf_20 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_x_{method}.csv', index_col=0)\n",
    "    ydf_20 = pd.read_csv(f'{RESULTS_DIR}/{experiment_id}/change_slope/MeanError_y_{method}.csv', index_col=0)\n",
    "    for df in [xdf_20, ydf_20]:\n",
    "        df.loc[:, 'correlation_coefficient'] = 0.20\n",
    "\n",
    "    full_xdf = xdf_75.append([xdf_00, xdf_20, xdf_30, xdf_50], ignore_index=True)\n",
    "    full_ydf = ydf_75.append([xdf_00, ydf_20, ydf_30, ydf_50], ignore_index=True)\n",
    "    full_df = full_xdf.append(full_ydf, ignore_index=True)\n",
    "    sns.set(rc={'figure.figsize':(20,10)})\n",
    "    sns.displot(data=full_df,\n",
    "                x='mean_error',\n",
    "                hue=\"correlation_coefficient\",\n",
    "                col='modified_attribute',\n",
    "                fill=True,\n",
    "                kind='kde',\n",
    "                palette='crest',\n",
    "                alpha=0.1,\n",
    "                linewidth=2.0,\n",
    "                legend=False)\n",
    "    plt.suptitle(method, y=0.99, x=0.54)\n",
    "plot_distributions(method) 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}