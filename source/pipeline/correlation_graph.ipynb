{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ruyogagp/medical_interpretability')\n",
    "import numpy as np\n",
    "from pysurvival.models import BaseModel\n",
    "from pysurvival import utils\n",
    "import scipy\n",
    "import pandas as pandas\n",
    "import copy\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from source.utils import create_correlated_var\n",
    "from pysurvival.models.simulations import SimulationModel\n",
    "from lifelines import CoxPHFitter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "from cga import cga\n",
    "from itertools import cycle\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Callable, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def fit_coxph(df):\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df, 'time', 'event')\n",
    "    cph.print_summary()\n",
    "\n",
    "def fit_coxph_norm(df):\n",
    "    standard_scaler = StandardScaler()\n",
    "    for col in df.columns:\n",
    "        if col == 'time' or col == 'event':\n",
    "            pass\n",
    "        df[col] = standard_scaler.fit_transform(df[[col]])\n",
    "    cph = CoxPHFitter()\n",
    "    cph.fit(df, 'time', 'event')\n",
    "    cph.print_summary()\n",
    "\n",
    "\n",
    "def save_orig(df, name, output_dir):\n",
    "    train, valid = train_test_split(df, test_size=0.3)\n",
    "    train.to_csv(\n",
    "        f\"{output_dir}/{name}_train.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    valid.to_csv(\n",
    "        f\"{output_dir}/{name}_valid.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    print(f\"Saved {output_dir}/{name}_train.csv\")\n",
    "    print(f\"Saved {output_dir}/{name}_valid.csv\")\n",
    "\n",
    "def df2csv(\n",
    "        df: pd.DataFrame,\n",
    "        name: str,\n",
    "        output_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes csv given a dataframe + name\n",
    "    \"\"\"\n",
    "    train, valid = train_test_split(df, test_size=0.3)\n",
    "    train.to_csv(\n",
    "        f\"{output_dir}/{name}_train_details.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    valid.to_csv(\n",
    "        f\"{output_dir}/{name}_valid_details.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    train_df = train.loc[:, ['x_orig', 'y_orig', 'time_orig', 'event_orig']]\n",
    "    valid_df = valid.loc[:, ['x_orig', 'y_orig', 'time_orig', 'event_orig']]\n",
    "    train_df.rename(columns=dict(x_orig='x',\n",
    "                                 y_orig='y',\n",
    "                                 time_orig='time',\n",
    "                                 event_orig='event'), inplace=True)\n",
    "\n",
    "    valid_df.rename(columns=dict(x_orig='x',\n",
    "                                 y_orig='y',\n",
    "                                 time_orig='time',\n",
    "                                 event_orig='event'), inplace=True)\n",
    "    train_df.to_csv(\n",
    "        f\"{output_dir}/{name}_train.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    valid_df.to_csv(\n",
    "        f\"{output_dir}/{name}_valid.csv\",\n",
    "        index=False,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Model with correlations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SimulationModelWithCorrelations(SimulationModel):\n",
    "    \"\"\"\n",
    "    Subclasses `SimulationModel` to generated data from an predefined\n",
    "    risk factor.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_data(self,\n",
    "                      df: pd.DataFrame,\n",
    "                      feature_weights: list,\n",
    "                      feature_names: list,\n",
    "                      include_hazard: bool = False\n",
    "                      ):\n",
    "\n",
    "        def risk_function(x_std, feature_weights):\n",
    "            \"\"\" Calculating the risk function based on the given risk type \"\"\"\n",
    "\n",
    "            # Dot product\n",
    "            risk = np.dot(x_std, feature_weights )\n",
    "\n",
    "            # Choosing the type of risk\n",
    "            if self.risk_type.lower() == 'linear' :\n",
    "                return risk.reshape(-1, 1)\n",
    "\n",
    "            elif self.risk_type.lower() == 'square' :\n",
    "                risk = np.square(risk*self.risk_parameter)\n",
    "\n",
    "\n",
    "            elif self.risk_type.lower() == 'gaussian' :\n",
    "                risk = np.square(risk)\n",
    "                risk = np.exp( - risk*self.risk_parameter)\n",
    "\n",
    "            return risk.reshape(-1, 1)\n",
    "\n",
    "        input_data = df.loc[:, feature_names].to_numpy()\n",
    "        self.dataset = copy.deepcopy(df)\n",
    "        num_samples = input_data.shape[0]\n",
    "        X_std = self.scaler.fit_transform(input_data)\n",
    "        BX = risk_function(X_std, feature_weights)\n",
    "\n",
    "        # Building the survival times\n",
    "        T = self.time_function(BX)\n",
    "        C = np.random.normal(loc=self.censored_parameter, scale=5, size=num_samples)\n",
    "        C = np.maximum(C, 0.0)\n",
    "        time = np.minimum(T, C)\n",
    "        E = 1.0 * (T == time)\n",
    "\n",
    "        # Building dataset\n",
    "        self.dataset = copy.deepcopy(df)\n",
    "        self.dataset['time'] = time\n",
    "        self.dataset['event'] = E\n",
    "        if include_hazard:\n",
    "            self.dataset['hazard'] = BX\n",
    "\n",
    "        # Building the time axis and time buckets\n",
    "        self.times = np.linspace(0.0, max(self.dataset[\"time\"]), self.bins)\n",
    "        self.get_time_buckets()\n",
    "\n",
    "        # Building baseline functions\n",
    "        self.baseline_hazard = self.hazard_function(self.times, 0)\n",
    "        self.baseline_survival = self.survival_function(self.times, 0)\n",
    "\n",
    "        # Printing summary message\n",
    "        message_to_print = \"Number of data-points: {} - Number of events: {}\"\n",
    "        print(message_to_print.format(num_samples, sum(E)))\n",
    "        return self.dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation Case Graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "@cga.node\n",
    "def correlation_000(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    :param x: exisiting data to correlate\n",
    "    :param coeff: correlation coefficient\n",
    "    :param noise: noise variable\n",
    "    :return: variable correlated by coeff to the exisiting variable x\n",
    "    \"\"\"\n",
    "    correlate = create_correlated_var(x,\n",
    "                                      mu=np.mean(x),\n",
    "                                      sd=np.std(x),\n",
    "                                      empirical=True,\n",
    "                                      r=0.000)\n",
    "    return correlate + noise\n",
    "\n",
    "@cga.node\n",
    "def correlation_025(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    :param x: exisiting data to correlate\n",
    "    :param coeff: correlation coefficient\n",
    "    :param noise: noise variable\n",
    "    :return: variable correlated by coeff to the exisiting variable x\n",
    "    \"\"\"\n",
    "    correlate = create_correlated_var(x,\n",
    "                                      mu=np.mean(x),\n",
    "                                      sd=np.std(x),\n",
    "                                      empirical=True,\n",
    "                                      r=0.250)\n",
    "    return correlate + noise\n",
    "\n",
    "@cga.node\n",
    "def correlation_050(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    :param x: exisiting data to correlate\n",
    "    :param coeff: correlation coefficient\n",
    "    :param noise: noise variable\n",
    "    :return: variable correlated by coeff to the exisiting variable x\n",
    "    \"\"\"\n",
    "    correlate = create_correlated_var(x,\n",
    "                                      mu=np.mean(x),\n",
    "                                      sd=np.std(x),\n",
    "                                      empirical=True,\n",
    "                                      r=0.500)\n",
    "    return correlate + noise\n",
    "\n",
    "@cga.node\n",
    "def correlation_075(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    :param x: exisiting data to correlate\n",
    "    :param coeff: correlation coefficient\n",
    "    :param noise: noise variable\n",
    "    :return: variable correlated by coeff to the exisiting variable x\n",
    "    \"\"\"\n",
    "    correlate = create_correlated_var(x,\n",
    "                                      mu=np.mean(x),\n",
    "                                      sd=np.std(x),\n",
    "                                      empirical=True,\n",
    "                                      r=0.750)\n",
    "    return correlate + noise\n",
    "\n",
    "@cga.node\n",
    "def sample_random_normal(noise:float)->float:\n",
    "    \"\"\"\n",
    "    :param n: sample size\n",
    "    :param noise: noise variable\n",
    "    :return: random normal variable\n",
    "    \"\"\"\n",
    "    return np.random.normal(size=100) + noise\n",
    "\n",
    "@cga.node\n",
    "def correlation_coefficient(coeff:float) -> float:\n",
    "    return coeff\n",
    "\n",
    "class CorrelationCaseGraph(cga.Graph):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        causal graph for correlation case\n",
    "        :param n: number of data points\n",
    "        :param coeff: desired correlation coefficient between the two variables\n",
    "        \"\"\"\n",
    "        noise = cga.node(lambda: np.random.normal(scale=0.1, size=100))\n",
    "        rnorm_vector = cga.node(lambda: np.random.normal(size=100))\n",
    "        self.rnorm = rnorm_vector(name=\"rnorm\")\n",
    "        self.noise0 = noise(name=\"noise0\")\n",
    "        self.noise1 = noise(name=\"noise1\")\n",
    "        self.noise2 = noise(name=\"noise2\")\n",
    "        self.noise3 = noise(name=\"noise3\")\n",
    "        self.noise4 = noise(name=\"noise4\")\n",
    "        self.feature0 = sample_random_normal(self.noise0, name='feature0')\n",
    "        self.feature1 = correlation_000(self.feature0, self.noise1, name='feature1')\n",
    "        self.feature2 = correlation_025(self.feature0, self.noise2, name='feature2')\n",
    "        self.feature3 = correlation_050(self.feature0, self.noise3, name='feature3')\n",
    "        self.feature4 = correlation_075(self.feature0, self.noise4, name='feature4')\n",
    "        super().__init__([self.feature0, self.feature1, self.feature2, self.feature3, self.feature4])\n",
    "\n",
    "    def get_interventions(self,\n",
    "                          sim: SimulationModelWithCorrelations,\n",
    "                          n_iterations: int,\n",
    "                          feature_weights: list,\n",
    "                          ) -> pd.DataFrame:\n",
    "        data = None\n",
    "        for node in [self.noise0, self.noise1, self.noise2, self.noise3, self.noise4]:\n",
    "            for _ in tqdm.trange(n_iterations, desc=f\"Intervention {node.name}\"):\n",
    "                # resample noise\n",
    "                orig, interventions, *_ = self.sample_do(action=cga.Resample(node), n_samples=100)\n",
    "                row = {'modified_attribute': [node.name] * 100}\n",
    "                # add orig + do to the dictionary\n",
    "                row.update({\n",
    "                    n.name + \"_orig\": v\n",
    "                    for n, v in orig.items()\n",
    "                })\n",
    "                for idx, intervention in enumerate(interventions):\n",
    "                    row.update({\n",
    "                        n.name + f\"_intervention{idx}\": v\n",
    "                        for n, v in intervention.items()})\n",
    "\n",
    "                data = row if data is None else data\n",
    "                for key in row.keys():\n",
    "                    row[key] = row[key].tolist() if isinstance(row[key], np.ndarray) else row[key]\n",
    "                    data[key].extend(row[key])\n",
    "        intervention_df = pd.DataFrame(data)\n",
    "\n",
    "        orig_cols = ['feature0_orig', 'feature1_orig', 'feature2_orig', 'feature3_orig', 'feature4_orig']\n",
    "        modified_attributes = [f'noise{i}' for i in range(len(orig_cols))]\n",
    "        orig_df = sim.generate_data(intervention_df, feature_names=orig_cols,\n",
    "                                    feature_weights=feature_weights,\n",
    "                                    include_hazard=True)\n",
    "\n",
    "        # split by modified attribute, to get the input data for attribution\n",
    "        attribution_dfs = self.slice_dataframe(orig_df, modified_attributes)\n",
    "\n",
    "        intervention_df['hazard_orig'] = orig_df.hazard\n",
    "        intervention_df['event_orig'] = orig_df.event\n",
    "        intervention_df['time_orig'] = orig_df.time\n",
    "\n",
    "        return attribution_dfs, intervention_df\n",
    "\n",
    "    def slice_dataframe(self, orig_df, modified_attributes):\n",
    "        df_list = []\n",
    "        for modified_attribute in modified_attributes:\n",
    "            df = orig_df.loc[orig_df.modified_attribute==modified_attribute]\\\n",
    "                      .loc[:, ['feature0_orig', 'feature1_orig', 'feature2_orig', 'feature3_orig', 'feature4_orig', 'time', 'event']]\\\n",
    "                      .rename(columns=dict(feature0_orig='feature0',\n",
    "                                           feature1_orig='feature1',\n",
    "                                           feature2_orig='feature2',\n",
    "                                           feature3_orig='feature3',\n",
    "                                           feature4_orig='feature4'))\n",
    "            df_list.append(df)\n",
    "        return df_list\n",
    "\n",
    "    def test_intervention(self, n_iterations):\n",
    "        for node in [self.noise_x]:\n",
    "            for _ in tqdm.trange(n_iterations, desc=f\"Intervention {node.name}\"):\n",
    "                # resample noise\n",
    "                orig, intervention0, intervention1 = self.sample_do(action=cga.Resample(node))\n",
    "        return orig, intervention0, intervention1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample from Graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sample features\n",
    "data = None\n",
    "correlation_graph = CorrelationCaseGraph()\n",
    "for _ in tqdm.trange(100, desc='sampling'):\n",
    "    result = correlation_graph.sample()\n",
    "    data = result if data is None else data\n",
    "    for key in result.keys():\n",
    "        result[key] = result[key].tolist() if isinstance(result[key], np.ndarray) else result[key]\n",
    "        data[key].extend(result[key])\n",
    "del data[correlation_graph.noise0]\n",
    "del data[correlation_graph.noise1]\n",
    "del data[correlation_graph.noise2]\n",
    "del data[correlation_graph.noise3]\n",
    "del data[correlation_graph.noise4]\n",
    "\n",
    "# Generate data\n",
    "training_features = pd.DataFrame(data)\n",
    "sim = SimulationModelWithCorrelations(risk_type='linear', alpha=1.0, beta=5.0, censored_parameter=5.0, survival_distribution='weibull')\n",
    "feature_weights = [np.log(2), np.log(1.5), np.log(1.5), np.log(1.5), np.log(1.5)]\n",
    "feature_names = [correlation_graph.feature0, correlation_graph.feature1, correlation_graph.feature2, correlation_graph.feature3, correlation_graph.feature4]\n",
    "training_df = sim.generate_data(training_features, feature_weights=feature_weights, feature_names=feature_names)\n",
    "\n",
    "# Check correlations\n",
    "training_df.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_df.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Training Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "directory = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities'\n",
    "save_orig(training_df, name='multi_correlation', output_dir=directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resample Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Resample from graph\n",
    "correlation_graph = CorrelationCaseGraph()\n",
    "feature_weights = [np.log(2), np.log(1.5), np.log(1.5), np.log(1.5), np.log(1.5)]\n",
    "sim = SimulationModelWithCorrelations(risk_type='linear', alpha=1.0, beta=5.0, censored_parameter=5.0, survival_distribution='weibull')\n",
    "attribution_dfs, intervention_details = correlation_graph.get_interventions(sim=sim, n_iterations=30, feature_weights=feature_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Attribution Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = 'multi-correlation'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, df in enumerate(attribution_dfs):\n",
    "    df.to_csv(f'{directory}/{experiment_name}_attribute_feature{idx}.csv', index=False)\n",
    "intervention_details.to_csv(f'{directory}/{experiment_name}_attribute_details.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intervention_details"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.read_csv(f'{directory}/{experiment_name}_attribute_feature0.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simpsons Paradox Graph"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_simpsons_paradox(\n",
    "        p: float = 2,\n",
    "        q: float = 1,\n",
    "        n: float = 500,\n",
    "        n_groups: int = 5,\n",
    "):\n",
    "\n",
    "    k = np.random.choice(5, size=n)\n",
    "    scaling = np.random.normal(size=n)\n",
    "\n",
    "    noise_x = np.random.normal(scale=0.25, size=n)\n",
    "    noise_y = np.random.normal(scale=0.25, size=n)\n",
    "    y = scaling * np.sin(p / q) + k + noise_y\n",
    "    x = scaling * np.cos(p / q) + k + noise_x\n",
    "    return x, y\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def ifnone(maybe: Optional[T], default: T) -> T:\n",
    "    if maybe is None:\n",
    "        return default\n",
    "    else:\n",
    "        return maybe\n",
    "\n",
    "\n",
    "@cga.node\n",
    "def simpson_x(\n",
    "        scaling: float,\n",
    "        group: float,\n",
    "        noise: float,\n",
    ") -> float:\n",
    "    return scaling * np.cos(2 / 1) + group + noise\n",
    "\n",
    "\n",
    "@cga.node\n",
    "def simpson_y(\n",
    "        scaling: float,\n",
    "        group: float,\n",
    "        noise: float,\n",
    ") -> float:\n",
    "    return scaling * np.sin(2 / 1) + group + noise\n",
    "\n",
    "\n",
    "@cga.node\n",
    "def simpson_hazzard(\n",
    "        scaling: float,\n",
    "        group: float,\n",
    ") -> float:\n",
    "    return np.where(group == 2, scaling, -scaling).item()\n",
    "\n",
    "\n",
    "class SimpsonsParadoxGraph(cga.Graph):\n",
    "    def __init__(self):\n",
    "        # define functions\n",
    "        noise = cga.node(lambda: np.random.normal(scale=0.27))\n",
    "        get_group = cga.node(lambda: np.random.choice(5))\n",
    "        get_scaling = cga.node(lambda: np.random.normal())\n",
    "\n",
    "        self.noise_x = noise(name=\"noise_x\")\n",
    "        self.noise_y = noise(name=\"noise_y\")\n",
    "        self.group = get_group(name=\"group\")\n",
    "\n",
    "        self.scaling = get_scaling(name=\"scaling\")\n",
    "\n",
    "        self.x = simpson_x(self.scaling, self.group, self.noise_x, name=\"x\")\n",
    "        self.y = simpson_y(self.scaling, self.group, self.noise_y, name=\"y\")\n",
    "\n",
    "        self.hazzard = simpson_hazzard(self.scaling, self.group, name=\"hazzard\")\n",
    "        super().__init__([self.x, self.y, self.hazzard])\n",
    "\n",
    "    def get_interventions(self, g, sim, n_samples):\n",
    "        data = []\n",
    "        for node in [g.noise_x, g.noise_y]:\n",
    "            for _ in tqdm.auto.trange(n_samples,\n",
    "                                      desc=f\"Intervention {node.name}\"):\n",
    "                orig, interventions = g.sample_do(\n",
    "                    action=cga.Resample(node),\n",
    "                    n_samples=100,\n",
    "                )\n",
    "                row = {'modified_attribute': node.name}\n",
    "                row.update({\n",
    "                    n.name + \"_orig\": v\n",
    "                    for n, v in orig.items()\n",
    "                })\n",
    "                for idx, intervention in enumerate(interventions):\n",
    "                    row.update({\n",
    "                        n.name + f\"_intervention{idx}\": v\n",
    "                        for n, v in intervention.items()})\n",
    "                data.append(row)\n",
    "            df = pd.DataFrame(data)\n",
    "        sim_df = sim.generate_data(df, hazzard_column='hazzard_orig')\n",
    "        df['event_orig'] = sim_df.event\n",
    "        df['time_orig'] = sim_df.time\n",
    "\n",
    "        # separate x and y to build attribute dataframe\n",
    "        xdf = df.loc[df.modified_attribute=='noise_x']\\\n",
    "                  .loc[:, ['x_orig', 'y_orig', 'time_orig', 'event_orig']]\\\n",
    "            .rename(columns=dict(x_orig='x', y_orig='y', time_orig='time', event_orig='event'))\n",
    "        ydf = df.loc[df.modified_attribute=='noise_y']\\\n",
    "                  .loc[:, ['x_orig', 'y_orig', 'time_orig', 'event_orig']]\\\n",
    "            .rename(columns=dict(x_orig='x', y_orig='y', time_orig='time', event_orig='event'))\n",
    "\n",
    "        return xdf, ydf, df\n",
    "\n",
    "\n",
    "    def transform(\n",
    "            self,\n",
    "            dataset_row: pd.Series,\n",
    "            set_values: dict = {},\n",
    "            replace_nodes: dict = {},\n",
    "    ) -> pd.Series:\n",
    "        # print(dataset_row.keys())\n",
    "        result = self.sample(\n",
    "            set_values={\n",
    "                self.scaling: dataset_row[\"predictive0\"],\n",
    "                self.group: np.digitize(\n",
    "                    dataset_row[\"nonpredictive0\"], [-1.5, -1, 0, 1, 1.5]\n",
    "                ),\n",
    "            },\n",
    "            replace=replace_nodes,\n",
    "        )\n",
    "        return pd.Series(\n",
    "            index=[\"x\", \"y\", \"event\", \"time\"],\n",
    "            data=[\n",
    "                result[self.x],\n",
    "                result[self.y],\n",
    "                dataset_row[\"event\"],\n",
    "                dataset_row[\"time\"],\n",
    "            ],\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Model with Risk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SimulationModelWithRisk(SimulationModel):\n",
    "    \"\"\"\n",
    "    Subclasses `SimulationModel` to generated data from an predefined\n",
    "    risk factor.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_data(\n",
    "            self,\n",
    "            dataframe: pd.DataFrame,\n",
    "            hazzard_column=\"hazzard\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generating a dataset of simulated survival times from a given\n",
    "        distribution through the hazard function using the Cox model\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "\n",
    "        * `dataframe`: **pd.Dataframe** --\n",
    "            A pandas dataframe with a risk column.\n",
    "\n",
    "        * `hazzard_column`: **str** *(default=\"risk\")* --\n",
    "            Name of the risk column.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        * dataset: pandas.DataFrame\n",
    "            dataset of simulated survival times, event status and features\n",
    "\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        from pysurvival.models.simulations import SimulationModel\n",
    "\n",
    "        # Initializing the simulation model\n",
    "        sim = SimulationModel( survival_distribution = 'gompertz',\n",
    "                               risk_type = 'linear',\n",
    "                               censored_parameter = 5.0,\n",
    "                               alpha = 0.01,\n",
    "                               beta = 5., )\n",
    "\n",
    "        # Generating N Random samples\n",
    "        N = 1000\n",
    "        dataset = sim.generate_data(num_samples = N, num_features=5)\n",
    "\n",
    "        # Showing a few data-points\n",
    "        dataset.head()\n",
    "        \"\"\"\n",
    "\n",
    "        def risk_function(risk: np.ndarray) -> np.ndarray:\n",
    "            # Choosing the type of risk\n",
    "            if self.risk_type.lower() == \"linear\":\n",
    "                return risk.reshape(-1, 1)\n",
    "\n",
    "            elif self.risk_type.lower() == \"square\":\n",
    "                risk = np.square(risk * self.risk_parameter)\n",
    "\n",
    "            elif self.risk_type.lower() == \"gaussian\":\n",
    "                risk = np.square(risk)\n",
    "                risk = np.exp(-risk * self.risk_parameter)\n",
    "\n",
    "            return risk.reshape(-1, 1)\n",
    "\n",
    "        num_samples = len(dataframe)\n",
    "\n",
    "        BX = risk_function(np.array(dataframe[hazzard_column]))\n",
    "\n",
    "        # Building the survival times\n",
    "        T = self.time_function(BX)\n",
    "        C = np.random.normal(loc=self.censored_parameter, scale=5, size=num_samples)\n",
    "        C = np.maximum(C, 0.0)\n",
    "        time = np.minimum(T, C)\n",
    "        E = 1.0 * (T == time)\n",
    "\n",
    "        # Building dataset\n",
    "        self.dataset = copy.deepcopy(dataframe)\n",
    "        self.dataset[\"time\"] = time\n",
    "        self.dataset[\"event\"] = E\n",
    "\n",
    "        # Building the time axis and time buckets\n",
    "        self.times = np.linspace(0.0, max(self.dataset[\"time\"]), self.bins)\n",
    "        self.get_time_buckets()\n",
    "\n",
    "        # Building baseline functions\n",
    "        self.baseline_hazard = self.hazard_function(self.times, 0)\n",
    "        self.baseline_survival = self.survival_function(self.times, 0)\n",
    "\n",
    "        # Printing summary message\n",
    "        message_to_print = \"Number of data-points: {} - Number of events: {}\"\n",
    "        print(message_to_print.format(num_samples, sum(E)))\n",
    "        return self.dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "simpson_graph = SimpsonsParadoxGraph()\n",
    "gnx = simpson_graph.to_networkx()\n",
    "nx.draw_networkx(\n",
    "    gnx,\n",
    "    labels={n: n.name for n in gnx.nodes},\n",
    "    pos=nx.layout.spring_layout(gnx, k=3),\n",
    "    node_size=2_000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_samples = 10_000\n",
    "data = []\n",
    "for _ in tqdm.auto.trange(n_samples):\n",
    "    result = simpson_graph.sample()\n",
    "    data.append(\n",
    "        {\n",
    "            \"x\": result[simpson_graph.x],\n",
    "            \"y\": result[simpson_graph.y],\n",
    "            \"hazzard\": float(result[simpson_graph.hazzard]),\n",
    "            \"group\": result[simpson_graph.group],\n",
    "            \"scaling\": result[simpson_graph.scaling],\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(data)\n",
    "sim = SimulationModelWithRisk(risk_type='linear',\n",
    "                                      alpha=1.0,\n",
    "                                      beta=5.0,\n",
    "                                      censored_parameter=5.0,\n",
    "                                      survival_distribution='weibull')\n",
    "df = sim.generate_data(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "directory = '/data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities'\n",
    "training_df = df.loc[:, ['x', 'y', 'time', 'event']]\n",
    "save_orig(training_df, name='simpsons', output_dir=directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xdf, ydf, df = simpson_graph.get_interventions(simpson_graph, sim, 5000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xdf.to_csv(f'{directory}/simpsons_attribute_x.csv', index=False)\n",
    "ydf.to_csv(f'{directory}/simpsons_attribute_y.csv', index=False)\n",
    "df.to_csv(f'{directory}/simpsons_attribute_details.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval('attribute_x')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "@cga.node\n",
    "def relu(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    Rectified Linear unit `mul`\n",
    "    \"\"\"\n",
    "    out = x * (x > 0)\n",
    "    return out + noise\n",
    "\n",
    "@cga.node\n",
    "def tanh(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    Hyperbolic Tangent\n",
    "    \"\"\"\n",
    "    out = np.tanh(x)\n",
    "    return out + noise\n",
    "\n",
    "@cga.node\n",
    "def sigmoid(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out + noise\n",
    "\n",
    "@cga.node\n",
    "def square(x: float, noise:float) -> float:\n",
    "    \"\"\"\n",
    "    Squared\n",
    "    \"\"\"\n",
    "    out = np.square(x)\n",
    "    return out + noise\n",
    "\n",
    "@cga.node\n",
    "def sample_random_normal(noise:float)->float:\n",
    "    \"\"\"\n",
    "    :param n: sample size\n",
    "    :param noise: noise variable\n",
    "    :return: random normal variable\n",
    "    \"\"\"\n",
    "    return np.random.normal() + noise\n",
    "\n",
    "\n",
    "class NonLinearCaseGraph(cga.Graph):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        causal graph for correlation case\n",
    "        :param n: number of data points\n",
    "        :param coeff: desired correlation coefficient between the two variables\n",
    "        \"\"\"\n",
    "        noise = cga.node(lambda: np.random.normal(scale=0.1))\n",
    "        self.noise0 = noise(name=\"noise0\")\n",
    "        self.noise1 = noise(name=\"noise1\")\n",
    "        self.noise2 = noise(name=\"noise2\")\n",
    "        self.noise3 = noise(name=\"noise3\")\n",
    "        self.noise4 = noise(name=\"noise4\")\n",
    "        self.feature0 = sample_random_normal(self.noise0, name='feature0')\n",
    "        self.feature1 = relu(self.feature0, self.noise1, name='feature1')\n",
    "        self.feature2 = tanh(self.feature0, self.noise2, name='feature2')\n",
    "        self.feature3 = sigmoid(self.feature0, self.noise3, name='feature3')\n",
    "        self.feature4 = square(self.feature0, self.noise4, name='feature4')\n",
    "        super().__init__([self.feature0, self.feature1, self.feature2, self.feature3, self.feature4])\n",
    "\n",
    "    def get_interventions(self,\n",
    "                          graph: NonLinearCaseGraph,\n",
    "                          sim: SimulationModelWithCorrelations,\n",
    "                          n_samples: int,\n",
    "                          feature_weights: list,\n",
    "                          ) -> pd.DataFrame:\n",
    "        data = []\n",
    "        for node in [graph.noise0, graph.noise1, graph.noise2, graph.noise3, graph.noise4]:\n",
    "            for _ in tqdm.auto.trange(n_samples,\n",
    "                                      desc=f\"Intervention {node.name}\"):\n",
    "                orig, interventions = graph.sample_do(\n",
    "                    action=cga.Resample(node),\n",
    "                    n_samples=100,\n",
    "                )\n",
    "                row = {'modified_attribute': node.name}\n",
    "                row.update({\n",
    "                    n.name + \"_orig\": v\n",
    "                    for n, v in orig.items()\n",
    "                })\n",
    "                for idx, intervention in enumerate(interventions):\n",
    "                    row.update({\n",
    "                        n.name + f\"_intervention{idx}\": v\n",
    "                        for n, v in intervention.items()})\n",
    "                data.append(row)\n",
    "        intervention_df = pd.DataFrame(data)\n",
    "\n",
    "        orig_cols = ['feature0_orig', 'feature1_orig', 'feature2_orig', 'feature3_orig', 'feature4_orig']\n",
    "        modified_attributes = [f'noise{i}' for i in range(len(orig_cols))]\n",
    "        orig_df = sim.generate_data(intervention_df, feature_names=orig_cols,\n",
    "                                    feature_weights=feature_weights,\n",
    "                                    include_hazard=True)\n",
    "\n",
    "        # split by modified attribute, to get the input data for attribution\n",
    "        attribution_dfs = self.slice_dataframe(orig_df, modified_attributes)\n",
    "\n",
    "        intervention_df['hazard_orig'] = orig_df.hazard\n",
    "        intervention_df['event_orig'] = orig_df.event\n",
    "        intervention_df['time_orig'] = orig_df.time\n",
    "\n",
    "        return attribution_dfs, intervention_df\n",
    "\n",
    "    def slice_dataframe(self, orig_df, modified_attributes):\n",
    "        df_list = []\n",
    "        for modified_attribute in modified_attributes:\n",
    "            df = orig_df.loc[orig_df.modified_attribute==modified_attribute]\\\n",
    "                     .loc[:, ['feature0_orig', 'feature1_orig', 'feature2_orig', 'feature3_orig', 'feature4_orig', 'time', 'event']]\\\n",
    "                .rename(columns=dict(feature0_orig='feature0',\n",
    "                                     feature1_orig='feature1',\n",
    "                                     feature2_orig='feature2',\n",
    "                                     feature3_orig='feature3',\n",
    "                                     feature4_orig='feature4'))\n",
    "            df_list.append(df)\n",
    "        return df_list\n",
    "\n",
    "    def test_intervention(self, n_iterations):\n",
    "        for node in [self.noise_x]:\n",
    "            for _ in tqdm.trange(n_iterations, desc=f\"Intervention {node.name}\"):\n",
    "                # resample noise\n",
    "                orig, intervention0, intervention1 = self.sample_do(action=cga.Resample(node))\n",
    "        return orig, intervention0, intervention1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling: 100%|██████████| 10000/10000 [00:01<00:00, 6478.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data-points: 10000 - Number of events: 7830.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample features for training data\n",
    "nonlinear_graph = NonLinearCaseGraph()\n",
    "data = []\n",
    "for _ in tqdm.trange(10000, desc='sampling'):\n",
    "    result = nonlinear_graph.sample()\n",
    "    data.append({'feature0': result[nonlinear_graph.feature0],\n",
    "                 'feature1': result[nonlinear_graph.feature1],\n",
    "                 'feature2': result[nonlinear_graph.feature2],\n",
    "                 'feature3': result[nonlinear_graph.feature3],\n",
    "                 'feature4': result[nonlinear_graph.feature4]})\n",
    "\n",
    "# Generate data\n",
    "training_features = pd.DataFrame(data)\n",
    "sim = SimulationModelWithCorrelations(risk_type='linear', alpha=1.0, beta=5.0, censored_parameter=5.0, survival_distribution='weibull')\n",
    "feature_weights = [np.log(2), np.log(1.4), np.log(1.3), np.log(1.2), np.log(1.1)]\n",
    "feature_names = [f'feature{i}' for i in range(5)]\n",
    "training_df = sim.generate_data(training_features, feature_weights=feature_weights, feature_names=feature_names, include_hazard=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/nonlinear_train.csv\n",
      "Saved /data/analysis/ag-reils/ag-reils-shared/cardioRS/data/interpretability/resample_multiplicities/nonlinear_valid.csv\n"
     ]
    }
   ],
   "source": [
    "save_orig(training_df, name='nonlinear', output_dir=DATA_DIRECTORY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "Intervention noise0:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57c74200450b4e9a835db31219191dcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Intervention noise1:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e42ac9554a244d5aa66fec4fa409205"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Intervention noise2:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b51b4a06da474145ac7c27224355cc32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Intervention noise3:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e096fda4b234085a812bae478a9558a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Intervention noise4:   0%|          | 0/3000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e303d98db38c407c99b9ea83de769bf1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data-points: 15000 - Number of events: 11823.0\n"
     ]
    }
   ],
   "source": [
    "# Resample from graph\n",
    "sim = SimulationModelWithCorrelations(risk_type='linear', alpha=1.0, beta=5.0, censored_parameter=5.0, survival_distribution='weibull')\n",
    "attribution_dfs, intervention_details = nonlinear_graph.get_interventions(graph=nonlinear_graph, sim=sim, n_samples=3000, feature_weights=feature_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "experiment_name = 'nonlinear'\n",
    "for idx, df in enumerate(attribution_dfs):\n",
    "    df.to_csv(f'{DATA_DIRECTORY}/{experiment_name}_attribute_feature{idx}.csv', index=False)\n",
    "intervention_details.to_csv(f'{DATA_DIRECTORY}/{experiment_name}_attribute_details.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}